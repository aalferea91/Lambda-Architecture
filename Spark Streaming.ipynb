{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Streaming\n",
    "The following code has to be executed in the pyspark instance in the notebook of our EMR cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First configure the spark application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.jars.packages\": \"org.apache.spark/spark-sql-kafka-0-10_2.12/3.1.2\",\n",
    "        \"spark.pyspark.python\": \"python3\",\n",
    "        \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "        \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "        \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\",\n",
    "        \"spark.sql.streaming.checkpointLocation\": \"/tmp/\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data from our Kafka topic in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark\n",
    "      .readStream\n",
    "      .format(\"kafka\")\n",
    "      .option(\"kafka.bootstrap.servers\", 'ec2-3-95-18-154.compute-1.amazonaws.com:9092'], #here input the correct DNS of our kafka server\n",
    "      .option(\"subscribe\", \"views\")\n",
    "      .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the schema and apply it to our stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "pythonSchema=(\n",
    "    StructType()\n",
    "        .add('comment_id',StringType())\n",
    "        .add('user',StringType())\n",
    "        .add('timestamp',StringType())\n",
    "        .add('device',StringType())\n",
    "        .add('geo',StringType())\n",
    "        .add('minutes',DoubleType())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_views=(\n",
    "    df\n",
    "        .select(f.col('value').cast('String').alias('jsonData'))\n",
    "        .select(f.from_json('jsonData',pythonSchema).alias('views'))\n",
    "        .select('views.*')\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work with streaming data we have to create an stream object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream= (\n",
    "    df_views\n",
    "        .writeStream\n",
    "    .format('memory')\n",
    "    .queryName('views')\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table('views').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the stream is being processed, we can also update our mongodb collection for each row of our stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package('pymongo') #for pymongo to work in our pyspark notebook we need to install this\n",
    "sc.install_pypi_package('dnspython')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username='aalferea91'\n",
    "password='HVPk1tchDKf71SY9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SendToMongoDB_ForeachWriter:\n",
    "    \n",
    "    def open(self,partition_id,epoch_id):\n",
    "        import pymongo\n",
    "        \n",
    "        client=pymongo.MongoClient(connection = f'mongodb+srv://aalferea91:{password}@lambdaprojectcluster.epdco1f.mongodb.net/?retryWrites=true&w=majority')\n",
    "        db=client.social\n",
    "        self.comments_col=db.comments\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def process(self,row):\n",
    "        self.comments_col.update_one(\n",
    "            {'_id':str(row['comment_id'])},\n",
    "            {'$inc':{'views':1}},\n",
    "            upsert=True\n",
    "        )\n",
    "    \n",
    "    def close(self,err):\n",
    "        if err:\n",
    "            raise err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream=(\n",
    "    df_views\n",
    "        .writeStream\n",
    "        .foreach(SendToMongoDB_ForeachWriter())\n",
    "    .outputMode('append')\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last we will also use Spark to write the data from the stream in a bucket of S3 in order to have that as our analytical layer for our analysts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream=(\n",
    "    df_views\n",
    "        .withColumn('year',f.year('timestamp')),\n",
    "        .withColumn('month',f.month('timestamp')),\n",
    "        .writeStream\n",
    "        .format('parquet')\n",
    "        .outputMode('append')\n",
    "        .partitionBy('year','month')\n",
    "        .option('path','s3a://com.mbit.aalferea/views/') #here put the location of your bucket\n",
    "        .start()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
